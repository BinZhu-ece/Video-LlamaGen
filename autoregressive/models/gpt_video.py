# Modified from:
#   VQGAN:    https://github.com/CompVis/taming-transformers/blob/master/taming/modules/transformer/mingpt.py
#   DiT:      https://github.com/facebookresearch/DiT/blob/main/models.py  
#   nanoGPT:  https://github.com/karpathy/nanoGPT/blob/master/model.py
#   llama:    https://github.com/facebookresearch/llama/blob/main/llama/model.py
#   gpt-fast: https://github.com/pytorch-labs/gpt-fast/blob/main/model.py
#   PixArt:   https://github.com/PixArt-alpha/PixArt-alpha/blob/master/diffusion/model/nets/PixArt_blocks.py
from dataclasses import dataclass
from typing import Optional, List

import sys, os
# 获取当前文件的目录
current_dir = os.path.dirname(os.path.abspath(__file__))
# 向上回溯两级找到所需的目录
parent_dir = os.path.dirname(os.path.dirname(current_dir))
# 将找到的目录添加到sys.path
sys.path.append(parent_dir)
import torch
import torch.nn as nn
from torch.nn import functional as F
from utils.drop_path import DropPath
# from modules import PatchEmbed2D

def find_multiple(n: int, k: int):
    if n % k == 0:
        return n
    return n + k - (n % k)

@dataclass
class ModelArgs:
    dim: int = 4096
    n_layer: int = 32
    n_head: int = 32
    n_kv_head: Optional[int] = None
    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2
    ffn_dim_multiplier: Optional[float] = None
    rope_base: float = 10000
    norm_eps: float = 1e-5
    initializer_range: float = 0.02
    
    token_dropout_p: float = 0.1
    attn_dropout_p: float = 0.0
    resid_dropout_p: float = 0.1
    ffn_dropout_p: float = 0.1
    drop_path_rate: float = 0.0

    num_classes: int = 1000
    caption_dim: int = 2048
    class_dropout_prob: float = 0.1
    model_type: str = 't2v'

    vocab_size: int = 16384
    cls_token_num: int = 1
    block_size: int = 256
    max_batch_size: int = 32
    max_seq_len: int = 2048

    vae_embed_dim: int = 2048
    t_downsample_size: int = 4
    num_frames: int = 17
    shuffle_video_tokens: bool = True

#################################################################################
#                      Embedding Layers for Class Labels                        #
#################################################################################
class LabelEmbedder(nn.Module):
    """
    Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.
    """
    def __init__(self, num_classes, hidden_size, dropout_prob):
        super().__init__()
        use_cfg_embedding = dropout_prob > 0
        self.embedding_table = nn.Embedding(num_classes + use_cfg_embedding, hidden_size)
        self.num_classes = num_classes
        self.dropout_prob = dropout_prob

    def token_drop(self, labels, force_drop_ids=None):
        """
        Drops labels to enable classifier-free guidance.
        """
        if force_drop_ids is None:
            drop_ids = torch.rand(labels.shape[0], device=labels.device) < self.dropout_prob
        else:
            drop_ids = force_drop_ids == 1
        labels = torch.where(drop_ids, self.num_classes, labels)
        return labels

    def forward(self, labels, train, force_drop_ids=None):
        use_dropout = self.dropout_prob > 0
        if (train and use_dropout) or (force_drop_ids is not None):
            labels = self.token_drop(labels, force_drop_ids)
        embeddings = self.embedding_table(labels).unsqueeze(1)
        return embeddings


#################################################################################
#                      Embedding Layers for Text Feature                        #
#################################################################################
class CaptionEmbedder(nn.Module):
    """
    Embeds text caption into vector representations. Also handles label dropout for classifier-free guidance.
    """
    def __init__(self, in_channels, hidden_size, uncond_prob, token_num=120):
        super().__init__()
        self.cap_proj = MLP(in_features=in_channels, hidden_features=hidden_size, out_features=hidden_size)
        self.register_buffer("uncond_embedding", nn.Parameter(torch.randn(token_num, in_channels) / in_channels ** 0.5))
        self.uncond_prob = uncond_prob

    def token_drop(self, caption, force_drop_ids=None):
        """
        Drops labels to enable classifier-free guidance.
        """
        if force_drop_ids is None:
            drop_ids = torch.rand(caption.shape[0], device=caption.device) < self.uncond_prob
        else:
            drop_ids = force_drop_ids == 1
        caption = torch.where(drop_ids[:, None, None], self.uncond_embedding, caption)
        return caption

    def forward(self, caption, train, force_drop_ids=None):
        use_dropout = self.uncond_prob > 0
        if (train and use_dropout) or (force_drop_ids is not None):
            caption = self.token_drop(caption, force_drop_ids)
        embeddings = self.cap_proj(caption)
        return embeddings


class MLP(nn.Module):
    def __init__(self, in_features, hidden_features, out_features):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features, bias=False)
        self.act = nn.GELU(approximate='tanh')
        self.fc2 = nn.Linear(hidden_features, out_features, bias=False)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.fc2(x)
        return x


#################################################################################
#                                  GPT Model                                    #
#################################################################################
class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        return x * torch.rsqrt(torch.mean(x * x, dim=-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float()).type_as(x)
        return output * self.weight


class FeedForward(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        hidden_dim = 4 * config.dim
        hidden_dim = int(2 * hidden_dim / 3)
        # custom dim factor multiplier
        if config.ffn_dim_multiplier is not None:
            hidden_dim = int(config.ffn_dim_multiplier * hidden_dim)
        hidden_dim = find_multiple(hidden_dim, config.multiple_of)

        self.w1 = nn.Linear(config.dim, hidden_dim, bias=False)
        self.w3 = nn.Linear(config.dim, hidden_dim, bias=False)
        self.w2 = nn.Linear(hidden_dim, config.dim, bias=False)
        self.ffn_dropout = nn.Dropout(config.ffn_dropout_p)

    def forward(self, x):
        return self.ffn_dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))


class KVCache(nn.Module):
    def __init__(self, max_batch_size, max_seq_length, n_head, head_dim, dtype):
        super().__init__()
        cache_shape = (max_batch_size, n_head, max_seq_length, head_dim)
        self.register_buffer('k_cache', torch.zeros(cache_shape, dtype=dtype))
        self.register_buffer('v_cache', torch.zeros(cache_shape, dtype=dtype))

    def update(self, input_pos, k_val, v_val):
        # input_pos: [S], k_val: [B, H, S, D]
        assert input_pos.shape[0] == k_val.shape[2]
        k_out = self.k_cache
        v_out = self.v_cache
        k_out[:, :, input_pos] = k_val
        v_out[:, :, input_pos] = v_val

        return k_out, v_out


class Attention(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        assert config.dim % config.n_head == 0
        self.dim = config.dim
        self.head_dim = config.dim // config.n_head
        self.n_head = config.n_head
        self.n_kv_head = config.n_kv_head if config.n_kv_head is not None else config.n_head
        total_kv_dim = (self.n_head + 2 * self.n_kv_head) * self.head_dim

        # key, query, value projections for all heads, but in a batch
        self.wqkv = nn.Linear(config.dim, total_kv_dim, bias=False)
        self.wo = nn.Linear(config.dim, config.dim, bias=False)
        self.kv_cache = None

        # regularization
        self.attn_dropout_p = config.attn_dropout_p
        self.resid_dropout = nn.Dropout(config.resid_dropout_p)

    def forward(
        self, x: torch.Tensor, freqs_cis: torch.Tensor = None, 
        input_pos: Optional[torch.Tensor] = None, 
        mask: Optional[torch.Tensor] = None
    ):
        bsz, seqlen, _ = x.shape
        kv_size = self.n_kv_head * self.head_dim
        xq, xk, xv = self.wqkv(x).split([self.dim, kv_size, kv_size], dim=-1)

        xq = xq.view(bsz, seqlen, self.n_head, self.head_dim)
        xk = xk.view(bsz, seqlen, self.n_kv_head, self.head_dim)
        xv = xv.view(bsz, seqlen, self.n_kv_head, self.head_dim)
        
        xq = apply_rotary_emb(xq, freqs_cis)
        xk = apply_rotary_emb(xk, freqs_cis)

        xq, xk, xv = map(lambda x: x.transpose(1, 2), (xq, xk, xv))

        if self.kv_cache is not None:
            keys, values = self.kv_cache.update(input_pos, xk, xv)
        else:
            keys, values = xk, xv
        keys = keys.repeat_interleave(self.n_head // self.n_kv_head, dim=1)
        values = values.repeat_interleave(self.n_head // self.n_kv_head, dim=1)

        output = F.scaled_dot_product_attention(
            xq, keys, values, 
            attn_mask=mask, 
            is_causal=True if mask is None else False, # is_causal=False is for KV cache
            dropout_p=self.attn_dropout_p if self.training else 0)            
        
        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, self.dim)

        output = self.resid_dropout(self.wo(output))
        return output


class TransformerBlock(nn.Module):
    def __init__(self, config: ModelArgs, drop_path: float):
        super().__init__()
        self.attention = Attention(config)
        self.feed_forward = FeedForward(config)
        self.attention_norm = RMSNorm(config.dim, eps=config.norm_eps)
        self.ffn_norm = RMSNorm(config.dim, eps=config.norm_eps)
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()

    def forward(
        self, x: torch.Tensor, freqs_cis: torch.Tensor, start_pos: int, mask: Optional[torch.Tensor] = None):
        h = x + self.drop_path(self.attention(self.attention_norm(x), freqs_cis, start_pos, mask))
        out = h + self.drop_path(self.feed_forward(self.ffn_norm(h)))
        return out


class Transformer(nn.Module):
    def __init__(self, config: ModelArgs):
        super().__init__()
        self.config = config
        self.vocab_size = config.vocab_size
        self.n_layer = config.n_layer
        self.block_size = config.block_size
        self.num_classes = config.num_classes
        self.model_type = config.model_type
        self.cls_token_num = config.cls_token_num
        self.num_frames = config.num_frames
        self.t_downsample_size = config.t_downsample_size
        # 2d rotary pos embedding
        grid_size = int(self.block_size ** 0.5)
        assert grid_size * grid_size == self.block_size

        # print(f'{self.model_type=}!')
        if self.model_type == 'c2i':
            self.cls_embedding = LabelEmbedder(config.num_classes, config.dim, config.class_dropout_prob)
        elif self.model_type == 't2i':
            self.cls_embedding = CaptionEmbedder(config.caption_dim, config.dim, config.class_dropout_prob)
            self.freqs_cis = precompute_freqs_cis_2d(grid_size, self.config.dim // self.config.n_head, self.config.rope_base, self.cls_token_num)
        elif self.model_type == 't2v':
            self.cls_embedding = CaptionEmbedder(config.caption_dim, config.dim, config.class_dropout_prob) # drop 0.1
            self.vae_latent_adapter = MLP(config.vae_embed_dim, config.dim, config.dim) # video latent adapter
            self.freqs_cis = precompute_freqs_cis_3d_video(grid_size,  (config.num_frames-1)//config.t_downsample_size+1 ,self.config.dim // self.config.n_head, self.config.rope_base, self.cls_token_num)
            self.vae_latent_adapter2 = MLP( config.dim, config.dim, config.vae_embed_dim) # video latent adapter
            self.shuffle_video_tokens=config.shuffle_video_tokens
        else:
            raise Exception("please check model type")
        self.tok_embeddings = nn.Embedding(config.vocab_size, config.dim)
        self.tok_dropout = nn.Dropout(config.token_dropout_p) # drop 0.1

        # transformer blocks
        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, config.n_layer)]
        self.layers = torch.nn.ModuleList()
        for layer_id in range(config.n_layer):
            self.layers.append(TransformerBlock(config, dpr[layer_id]))

        # output layer
        self.norm = RMSNorm(config.dim, eps=config.norm_eps)
        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)


        # KVCache
        self.max_batch_size = -1
        self.max_seq_length = -1

        self.initialize_weights()

    def initialize_weights(self):        
        # Initialize nn.Linear and nn.Embedding
        self.apply(self._init_weights)

        # Zero-out output layers:
        nn.init.constant_(self.output.weight, 0)

    def _init_weights(self, module):
        std = self.config.initializer_range
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=std)

    def setup_caches(self, max_batch_size, max_seq_length, dtype):
        # if self.max_seq_length >= max_seq_length and self.max_batch_size >= max_batch_size:
        #     return
        head_dim = self.config.dim // self.config.n_head
        max_seq_length = find_multiple(max_seq_length, 8)
        self.max_seq_length = max_seq_length
        self.max_batch_size = max_batch_size
        for b in self.layers:
            b.attention.kv_cache = KVCache(max_batch_size, max_seq_length, self.config.n_head, head_dim, dtype)

        causal_mask = torch.tril(torch.ones(self.max_seq_length, self.max_seq_length, dtype=torch.bool))
        self.causal_mask = causal_mask.unsqueeze(0).repeat(self.max_batch_size, 1, 1) # (bs, seq, seq)
        grid_size = int(self.config.block_size ** 0.5)
        assert grid_size * grid_size == self.block_size

        self.freqs_cis = precompute_freqs_cis_3d_video(
                                grid_size,  
                                (self.num_frames-1)//self.t_downsample_size+1 ,
                                self.config.dim // self.config.n_head, 
                                self.config.rope_base, 
                                self.cls_token_num
                                )
        # self.freqs_cis = precompute_freqs_cis_2d(grid_size, self.config.dim // self.config.n_head, self.config.rope_base, self.cls_token_num)

    def forward(
        self, 
        idx: Optional[torch.Tensor] = None,
        cond_idx: Optional[torch.Tensor] = None,  # cond_idx_or_embed
        input_pos:  Optional[torch.Tensor] = None, 
        targets: Optional[torch.Tensor] = None,
        mask: Optional[torch.Tensor] = None,
        valid: Optional[torch.Tensor] = None,
        cond_embed: Optional[torch.Tensor] = None,
        video_latent: Optional[torch.Tensor] = None,
        targets_video: Optional[torch.Tensor] = None,
    ):
        
        # assert video_latent is not None and targets_video is not None and self.shuffle_video_tokens==True
        if targets_video is not None and video_latent is not None and self.shuffle_video_tokens==True:
 
            noise = torch.rand(1, targets_video.shape[1]).expand(targets_video.shape[0], -1)  # noise in [0, 1], [N, L]
            # noise = targets_video[:,:,0]
            # noise = torch.rand(1, targets_video.shape[1], device=targets_video.device).expand(targets_video.shape[0], 1)  # noise in [0, 1], [N, L]
            ids_shuffle = torch.argsort(noise, dim=1).to(targets_video.device)   # ascend: small is keep, large is remove
            ids_restore = torch.argsort(ids_shuffle, dim=1).to(targets_video.device) 
            targets_video_shuffle =  torch.gather(targets_video, dim=1, index=ids_shuffle.unsqueeze(-1).expand(-1, -1, targets_video.shape[-1])) # # torch.Size([2, 1280, 2048])
            video_latent_shuffle = targets_video_shuffle[:, :-1].clone()  # torch.Size([2, 1279, 2048])
         
 
            # visual tokens positional encoding shuffle
            self.freqs_cis = self.freqs_cis.to(targets_video.device) # torch.Size([1400, 32, 2])
            visual_freqs_cis = self.freqs_cis[self.cls_token_num:]  # torch.Size([1280, 32, 2])
    
            # shuffle visual tokens 
            visual_freqs_cis_shuffle = torch.gather(visual_freqs_cis, dim=0, index=ids_shuffle[0].unsqueeze(-1).unsqueeze(-1).expand(-1, visual_freqs_cis.shape[-2],visual_freqs_cis.shape[-1]))  # torch.Size([1280, 32, 2])
            
            #=========== below will occur errors during training ========
            # self.freqs_cis[self.cls_token_num:] = visual_freqs_cis_shuffle[:] # torch.Size([1400, 32, 2])
            freqs_cis = torch.cat([self.freqs_cis[:self.cls_token_num], visual_freqs_cis_shuffle], dim=0) # torch.Size([1400, 32, 2])
            self.freqs_cis = freqs_cis.to(targets_video.device)
          
            # concat text embedding and video latent embeddings ( from Causual Video VAE)
            cond_embeddings = self.cls_embedding(cond_embed, train=self.training)[:,:self.cls_token_num] # projection
            token_embeddings = self.vae_latent_adapter(video_latent_shuffle) # projection
            token_embeddings = torch.cat((cond_embeddings, token_embeddings), dim=1)
            h = self.tok_dropout(token_embeddings) # torch.Size([2, 1399, 768])
 
        else:
            # import ipdb; ipdb.set_trace()
            if cond_embed is not None and video_latent is None : # prefill in inference
                # token_embeddings = self.cls_embedding(cond_idx, train=self.training)[:,:self.cls_token_num]
                token_embeddings = self.cls_embedding(cond_embed, train=self.training)[:,:self.cls_token_num] # projection
            
            elif cond_embed is None and video_latent is not None : # decode_n_tokens(kv cache) in inference
                token_embeddings = self.vae_latent_adapter(video_latent) # projection
                # token_embeddings = self.tok_embeddings(idx)

            bs = token_embeddings.shape[0]
            mask = self.causal_mask[:bs, None, input_pos]
            h = self.tok_dropout(token_embeddings)
            self.freqs_cis = self.freqs_cis
        
        if self.training:
            freqs_cis = self.freqs_cis[:token_embeddings.shape[1]]
        else:
            # freqs_cis = self.freqs_cis[:token_embeddings.shape[1]]
            freqs_cis = self.freqs_cis[input_pos]

        # import ipdb; ipdb.set_trace()
        # transformer blocks
        for layer in self.layers:
            h = layer(h, freqs_cis, input_pos, mask)
        
        # output layers
        h = self.norm(h)
        h = self.vae_latent_adapter2(h)


        # restore visual tokens  during inference
        if not self.training and  self.shuffle_video_tokens==True:
            #===================
            h_visual = h[:, self.cls_token_num:] # torch.Size([2, 1280, 2048])
            h_visual_restore = torch.gather(h_visual, 1, ids_restore.unsqueeze(-1).expand(-1, -1, h.shape[-1]))
            h[:, self.cls_token_num:] = h_visual_restore
        

        # t2v
        if self.training:
            if video_latent is not None or cond_embed is not None:
                pre_video_latents = h[:, self.cls_token_num - 1:].contiguous()
                loss = F.mse_loss(pre_video_latents, targets_video)
                # print(f'MSE loss: {loss.detach().cpu()}')
            # t2i
            elif video_latent is None and idx is not None:
                logits = self.output(h).float()
                if self.training:
                    logits = logits[:, self.cls_token_num - 1:].contiguous()
                # if we are given some desired targets also calculate the loss
                loss = None
                if valid is not None:
                    loss_all = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), reduction='none')
                    valid_all = valid[:,None].repeat(1, targets.shape[1]).view(-1)
                    loss = (loss_all * valid_all).sum() / max(valid_all.sum(), 1)
                elif targets is not None:
                    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) # default: mean
            # error
            else:
                raise ValueError("Either video_latent or idx should be None")

            return h, loss # , h[:, self.cls_token_num - 1:]
        else:
            return h, None # , h[:, self.cls_token_num - 1:]


    def get_fsdp_wrap_module_list(self) -> List[nn.Module]:
        return list(self.layers)

##################################################################################
#                      shuffle and restore video                                 #
##################################################################################
def shuffle_and_restore(N, L, dim, transformer):
    import torch

    # 假设 x 是你的输入张量，形状为 (N, L, dim)
    x = torch.randn(N, L, dim)  # 示例输入

    # Step 1: 使用 torch.argsort 打乱顺序
    # 生成随机顺序的索引
    random_indices = torch.argsort(torch.randn(N, L), dim=1)  # (N, L)

    # 使用这些索引打乱输入张量的顺序
    x_shuffled = torch.gather(x, 1, random_indices.unsqueeze(-1).expand(-1, -1, dim))

    # Step 2: 将打乱后的张量输入 Transformer
    # 这里假设 transformer 是一个你定义的 Transformer 模型
    output_shuffled = transformer(x_shuffled)

    # Step 3: 还原原始顺序
    # 获取逆排序索引
    _, inverse_indices = random_indices.sort(dim=1)

    # 使用这些索引还原经过 Transformer 处理后的张量的顺序
    output_original = torch.gather(output_shuffled, 1, inverse_indices.unsqueeze(-1).expand(-1, -1, dim))

    # 现在 output_original 的顺序与 x 的原始顺序相同


#################################################################################
#                      Rotary Positional Embedding Functions                    #
#################################################################################
# https://github.com/pytorch-labs/gpt-fast/blob/main/model.py 
def precompute_freqs_cis(seq_len: int, n_elem: int, base: int = 10000, cls_token_num=120):
    freqs = 1.0 / (base ** (torch.arange(0, n_elem, 2)[: (n_elem // 2)].float() / n_elem))
    t = torch.arange(seq_len, device=freqs.device)
    freqs = torch.outer(t, freqs) # (seq_len, head_dim // 2)
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    cache = torch.stack([freqs_cis.real, freqs_cis.imag], dim=-1) # (cls_token_num+seq_len, head_dim // 2, 2)
    cond_cache = torch.cat([torch.zeros(cls_token_num, n_elem // 2, 2), cache]) # (cls_token_num+seq_len, head_dim // 2, 2)
    return cond_cache 


def precompute_freqs_cis_2d(grid_size: int, n_elem: int, base: int = 10000, cls_token_num=120):
    # split the dimension into half, one for x and one for y
    half_dim = n_elem // 2
    freqs = 1.0 / (base ** (torch.arange(0, half_dim, 2)[: (half_dim // 2)].float() / half_dim))
    t = torch.arange(grid_size, device=freqs.device)
    freqs = torch.outer(t, freqs) # (grid_size, head_dim // 2)
    freqs_grid = torch.concat([
        freqs[:, None, :].expand(-1, grid_size, -1),
        freqs[None, :, :].expand(grid_size, -1, -1),
    ], dim=-1)  # (grid_size, grid_size, head_dim // 2)
    cache_grid = torch.stack([torch.cos(freqs_grid), torch.sin(freqs_grid)], dim=-1) # (grid_size, grid_size, head_dim // 2, 2)
    cache = cache_grid.flatten(0, 1)
    cond_cache = torch.cat([torch.zeros(cls_token_num, n_elem // 2, 2), cache]) # (cls_token_num+grid_size**2, head_dim // 2, 2)
    return cond_cache 

def precompute_freqs_cis_3d_video(grid_size: int, vae_t, n_elem: int, base: int = 10000, cls_token_num=120):
    # split the dimension into half, one for x and one for y
    half_dim = n_elem // 2
    freqs = 1.0 / (base ** (torch.arange(0, half_dim, 2)[: (half_dim // 2)].float() / half_dim))
    t = torch.arange(grid_size, device=freqs.device)
    freqs = torch.outer(t, freqs) # (grid_size, head_dim // 2)
    freqs_grid = torch.concat([
        freqs[:, None, :].expand(-1, grid_size, -1),
        freqs[None, :, :].expand(grid_size, -1, -1),
    ], dim=-1)  # (grid_size, grid_size, head_dim // 2)
    cache_grid = torch.stack([torch.cos(freqs_grid), torch.sin(freqs_grid)], dim=-1) # (grid_size, grid_size, head_dim // 2, 2)
    # import ipdb; ipdb.set_trace()
    cache = cache_grid.flatten(0, 1) # (grid_size*grid_size, head_dim // 2, 2)

    # 使用 repeat 方法来重复 t 次
    repeated_cache = cache.unsqueeze(0).repeat((vae_t, 1, 1, 1))  # 形状变为 (t, g * g, c, 2)
    # 然后使用 view 或 reshape 来展平前两维
    flattened_cache = repeated_cache.flatten(0, 1)  # 形状变为 (t * g * g, c, 2)

    cond_cache = torch.cat([torch.zeros(cls_token_num, n_elem // 2, 2), flattened_cache]) # (cls_token_num+grid_size**2, head_dim // 2, 2)
    return cond_cache 
    

def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor):
    # x: (bs, seq_len, n_head, head_dim)
    # freqs_cis (seq_len, head_dim // 2, 2)
    xshaped = x.float().reshape(*x.shape[:-1], -1, 2) # (bs, seq_len, n_head, head_dim//2, 2)
    freqs_cis = freqs_cis.view(1, xshaped.size(1), 1, xshaped.size(3), 2) # (1, seq_len, 1, head_dim//2, 2)
    x_out2 = torch.stack([
            xshaped[..., 0] * freqs_cis[..., 0] - xshaped[..., 1] * freqs_cis[..., 1],
            xshaped[..., 1] * freqs_cis[..., 0] + xshaped[..., 0] * freqs_cis[..., 1],
    ], dim=-1)
    x_out2 = x_out2.flatten(3)
    return x_out2.type_as(x)



#################################################################################
#                                GPT Configs                                    #
#################################################################################
### text-conditional
def GPT_7B(**kwargs):
    return Transformer(ModelArgs(n_layer=32, n_head=32, dim=4096, **kwargs)) # 6.6B

def GPT_3B(**kwargs):
    return Transformer(ModelArgs(n_layer=24, n_head=32, dim=3200, **kwargs)) # 3.1B

def GPT_1B(**kwargs):
    return Transformer(ModelArgs(n_layer=22, n_head=32, dim=2048, **kwargs)) # 1.2B

### class-conditional
def GPT_XXXL(**kwargs):
    return Transformer(ModelArgs(n_layer=48, n_head=40, dim=2560, **kwargs)) # 3.9B

def GPT_XXL(**kwargs):
    return Transformer(ModelArgs(n_layer=48, n_head=24, dim=1536, **kwargs)) # 1.4B

def GPT_XL(**kwargs):
    return Transformer(ModelArgs(n_layer=36, n_head=20, dim=1280, **kwargs)) # 775M

def GPT_L(**kwargs):
    return Transformer(ModelArgs(n_layer=24, n_head=16, dim=1024, **kwargs)) # 343M

def GPT_B(**kwargs):
    return Transformer(ModelArgs(n_layer=12, n_head=12, dim=768, **kwargs)) # 111M
        

GPT_models = {
    'GPT-B': GPT_B, 'GPT-L': GPT_L, 'GPT-XL': GPT_XL, 'GPT-XXL': GPT_XXL, 'GPT-XXXL': GPT_XXXL,
    'GPT-1B': GPT_1B, 'GPT-3B': GPT_3B, 'GPT-7B': GPT_7B, 
}


def video_latent_permute(video_latent):
    # (bs, dim ,t, h, w)
    # 使用 permute 调整维度顺序
    video_latent = video_latent.permute(0, 2, 3, 4, 1)  # (bs, t, h, w, dim)
    # 使用 reshape 将 t, h, w 维度展平
    video_latent = video_latent.reshape(video_latent.shape[0], -1, video_latent.shape[-1])  # (bs, t*h*w, dim)
    return video_latent

from einops import rearrange, repeat

import sys
current_file_path = os.path.abspath(__file__)
current_directory = os.path.dirname(current_file_path)
sys.path.append(os.path.join(current_directory, '..'))
sys.path.append(os.path.join('./','CausalVideoVAE'))
from dataset.transform import ToTensorVideo, TemporalRandomCrop, RandomHorizontalFlipVideo, CenterCropResizeVideo, LongSideResizeVideo, SpatialStrideCropVideo
# from CausalVideoVAE.causalvideovae.model import ae_norm, ae_denorm
from torchvision.transforms import Lambda
import argparse
import torchvision.transforms as transforms
from language.t5 import T5Embedder
from dataset.t2v import T2V_dataset
from torch.utils.data import Dataset, DataLoader
from tokenizer.tokenizer_image.vae_model import VAE_models

if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    # dataset & dataloader
    # parser.add_argument("--dataset", type=str, required=True)
    # parser.add_argument("--data", type=str, required='')
    parser.add_argument("--video_meta_info_file", type=str, default='/storage/zhubin/liuyihang/add_aes/output/sucai_aes_1000.json')
    # parser.add_argument("--sample_rate", type=int, default=1)
    # parser.add_argument("--cache_dir", type=str, required='')
    parser.add_argument("--t5-model-path", type=str, default='./pretrained_models/t5-ckpt')
    parser.add_argument("--t5-model-type", type=str, default='flan-t5-xl')
    parser.add_argument("--max_height", type=int, default=256)
    parser.add_argument("--max_width", type=int, default=256)
    parser.add_argument("--precision", type=str, default='bf16')
    parser.add_argument("--model_max_length", type=int, default=512)
    parser.add_argument("--start_frame_ind", type=int, default=25)
    parser.add_argument("--num_frames", type=int, default=17)
    parser.add_argument("--data_root", type=str, default='/storage/dataset')

    parser.add_argument("--image_size", type=int, default=256)
    parser.add_argument("--downsample-size", type=int, choices=[8, 16], default=16)
    parser.add_argument("--t-downsample-size", type=int, choices=[4, 8], default=4)

    parser.add_argument("--vae-model", type=str, choices=list(VAE_models.keys()), default="VAE-16")
    parser.add_argument("--vae-ckpt", type=str, default='/storage/zhubin/LlamaGen/CausalVideoVAE/vae_ckpt/432322048', help="ckpt path for vq model")
    parser.add_argument('--enable_tiling', action='store_true')
    parser.add_argument("--tile_overlap_factor", type=float, default=0.25)
    parser.add_argument("--t5-path", type=str, required=True) # t5 model path

    args = parser.parse_args()

    resize = [CenterCropResizeVideo((args.max_height, args.max_width)), ]
    # norm_fun = ae_norm[args.ae]
    norm_fun = Lambda(lambda x: 2. * x - 1.)
    transform = transforms.Compose([
        ToTensorVideo(),
        *resize, 
        # RandomHorizontalFlipVideo(p=0.5),  # in case their caption have position decription
        norm_fun
    ])
    from dataset.augmentation import center_crop_arr
    assert os.path.exists(args.t5_model_path)
    device='cuda'
    precision = {'none': torch.float32, 'bf16': torch.bfloat16, 'fp16': torch.float16}[args.precision]

    t5_xxl = T5Embedder(
        device=device, 
        local_cache=True, 
        cache_dir=args.t5_model_path, 
        dir_or_name=args.t5_model_type,
        torch_dtype=precision
    )

    dataset = T2V_dataset(args, transform, t5_xxl)
    dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0)
    # return dict(video=video, input_ids=input_ids, cond_mask=cond_mask)
    CausalVAEModel = VAE_models[args.vae_model] 
    vae = CausalVAEModel.from_pretrained(args.vae_ckpt)
    if args.enable_tiling:
        vae.enable_tiling()
        vae.tile_overlap_factor = args.tile_overlap_factor
    vae = vae.to(device) # .to(data_type)
    vae.eval()

    # import ipdb; ipdb.set_trace()
    latent_size = args.image_size // args.downsample_size
    for sample in dataloader:
        x, y, attn_mask, valid = sample['video_data']['video'], sample['video_data']['t5_feat_padding'], sample['video_data']['attn_mask'], sample['video_data']['valid']
       
        x = x.to(device)
        y = y.to(device)
        attn_mask = attn_mask.to(device)
        valid = valid.to(device)

        c_indices = y.reshape(y.shape[0], y.shape[-2], y.shape[-1]) # torch.Size([2, 120, 2048])
        """
        x: torch.Size([2, 3, 17, 256, 256])
        y: torch.Size([2, 1, 120, 2048])
        attn_mask: torch.Size([2, 1, 1400, 1400])
        valid: torch.Size([2])
        """
        with torch.no_grad():
            z = vae.encode(x).sample()
        # torch.Size([2, 2048, 5, 16, 16])
        video_latent =  z.flatten(2).transpose(1, 2) # (b, c, t, h, w)  ->  (b, t*h*w, c)
        cond_embed = c_indices

        model = GPT_models['GPT-B'](
            vocab_size=16384,
            block_size=latent_size**2,
            num_classes=1000,
            cls_token_num=120,
            resid_dropout_p=0.1,
            ffn_dropout_p=0.1,
            token_dropout_p=0.1,
            model_type='t2v',
            vae_embed_dim = 2048, 
            num_frames = args.num_frames,
            t_downsample_size = args.t_downsample_size, 
        ).to(device)

        attn_mask = attn_mask.reshape(attn_mask.shape[0], 1, attn_mask.shape[-2], attn_mask.shape[-1]) # (bs, n_head, seq_len, seq_len)
        
        # import ipdb; ipdb.set_trace()
        
        with torch.cuda.amp.autocast(dtype=precision):  
            _, loss = model(
                    idx = None,
                    cond_idx=None,
                    targets=None,
                    # cond_idx=c_indices,
                    # idx=z_indices[:, :-1],
                    # targets=z_indices,
                    mask=attn_mask[:, :, :-1, :-1], # torch.Size([2, 1, 1400, 1400])
                    valid=valid, # torch.Size([2])
                    video_latent=video_latent[:, :-1], # torch.Size([2, 1280, 2048])
                    targets_video=video_latent,
                    cond_embed=cond_embed, # torch.Size([2, 120, 2048])
                    )
           
            print(loss)

    """ 
    source  /storage/miniconda3/etc/profile.d/conda.sh 
    conda activate motionctrl
    export CUDA_VISIBLE_DEVICES=5
    cd /storage/zhubin/LlamaGen 
    python autoregressive/models/gpt_video.py --t5-path  /storage/zhubin/LlamaGen/dataset/storage_datasets_npy \
    --video_meta_info_file  /storage/zhubin/video_statistics_data/task1.5/Final_format_dataset_data_v2/step1.5_istock_final_815070_random_3000.json \
    --num_frames 1  
    
    """
